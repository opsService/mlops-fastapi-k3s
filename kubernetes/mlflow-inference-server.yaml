# mlops-fastapi-app/kubernetes/mlflow-inference-server.yaml (수정본)

---
# 1. MLflow Inference Server Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow-inference-server
  labels:
    app: mlflow-inference-server
spec:
  selector:
    matchLabels:
      app: mlflow-inference-server
  replicas: 1
  template:
    metadata:
      labels:
        app: mlflow-inference-server
    spec:
      containers:
      - name: inference-container
        image: localhost:5002/heedong/mlflow-inference-server:final-debug-v1
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 8000
        # ⭐ readinessProbe 추가
        readinessProbe:
          httpGet:
            path: /health # inference_server.py에 정의된 헬스 체크 엔드포인트
            port: 8000
          initialDelaySeconds: 10 # 컨테이너 시작 후 10초 대기
          periodSeconds: 5 # 5초마다 체크
          timeoutSeconds: 3 # 3초 내에 응답 없으면 실패
          failureThreshold: 3 # 3번 실패하면 Unready
        # ⭐ livenessProbe도 추가하는 것이 좋지만, 일단 readinessProbe부터
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30 # 컨테이너 시작 후 30초 대기
          periodSeconds: 10 # 10초마다 체크
          timeoutSeconds: 5 # 5초 내에 응답 없으면 실패
          failureThreshold: 3 # 3번 실패하면 컨테이너 재시작
        env:
        - name: MLFLOW_TRACKING_URI
          value: http://mlflow-tracking-service:5000
        - name: MLFLOW_S3_ENDPOINT_URL
          value: http://minio-service:9000
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: MINIO_ROOT_USER
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio-secret
              key: MINIO_ROOT_PASSWORD
        - name: INTERNAL_API_KEY
          valueFrom:
            secretKeyRef:
              name: fastapi-internal-api-key-secret # 이 Secret은 별도로 생성해야 합니다.
              key: API_KEY
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-secret # postgresql_deployment.yaml에서 사용하는 Secret과 동일하게
              key: POSTGRES_USER # Secret 내의 키 이름
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-secret # postgresql_deployment.yaml에서 사용하는 Secret과 동일하게
              key: POSTGRES_PASSWORD # Secret 내의 키 이름
        resources:
          requests:
            memory: "1Gi"
            cpu: "1"
          limits:
            memory: "2Gi"
            cpu: "2"
            nvidia.com/gpu: "1" # GPU 추론 시 필요하다면 주석 해제
        # ⭐ 여기에 livenessProbe와 readinessProbe를 추가합니다.
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 15
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 1
      # ⭐ GPU 추론 시 다음 주석 해제 (Pod Spec 레벨)
      runtimeClassName: nvidia

---
# 2. MLflow Inference Server Service (변경 없음)
apiVersion: v1
kind: Service
metadata:
  name: mlflow-inference-service
  labels:
    app: mlflow-inference-server
spec:
  selector:
    app: mlflow-inference-server
  ports:
    - protocol: TCP
      port: 8000
      targetPort: 8000
  type: ClusterIP